{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI traffic Counter Using YOLOv3 and OpenCV\n",
    "An AI traffic counter is implemented in this project to detect and track vehicles on a video stream and count those going through a defined line on each side of a highway. It utilizes the following two algorithms:\n",
    "- YOLO to detect objects on each of the video frames.\n",
    "- SORT to track those objects over different frames.\n",
    "This project explains implementation of an AI traffic counter using YOLOv3 and OpenCV to count elements in Realtime videos realtime object detection in real time video files. Computer vision is a huge part of the data science/AI domain which is substantially advanced over the last couple of years.\n",
    "\n",
    "\n",
    "## YOLO algorithm\n",
    "In recent years, deep learning algorithms are offering cutting-edge improved results for object detection. YOLO algorithm is one of the most popular Convolutional Neural Networks with a single end-to-end model that can perform object detection in real-time. YOLO stands for, You Only Look Once and is an algorithm developed by Joseph Redmon, et al. and first described in the 2015 paper titled “You Only Look Once: Unified, Real-Time Object Detection. The creation of the algorithm stemmed from the idea to place down a grid on the image and apply the image classification and localization algorithm to each of the grids.\n",
    "Here the YOLOv3, a refined design which uses predefined anchor boxes to improve bounding box, is utilized for object detection in new images. Source code and pre-trained models of YOLOv3 is available in the official DarkNet GitHub repository.\n",
    "\n",
    "\n",
    "#### Download the Pre-Trained Model\n",
    "The first step is to download the pre-trained model weights using the DarkNet code base on the COCO dataset and place them into current working directory with the filename “yolov3-tiny.weights”.\n",
    "\n",
    "- **YOLOv3-tiny Pre-trained Model Weights can be downloaded at** <a href=\"https://pjreddie.com/media/files/yolov3-tiny.weights\" target=\"_blank\">yolov3-tiny.weights(34 MB)</a>.\n",
    "\n",
    "\n",
    "## SORT Algorithms\n",
    "Simple Online and Realtime Tracking (SORT) is an implementation of tracking-by-detection framework, in which objects are detected in each frame and information of past and current frames are used to produce object identities on the fly. It is designed for online and real-time tracking applications. SORT was initially described in [this paper](http://arxiv.org/abs/1602.00763) by Alex Bewley et al.\n",
    "\n",
    "\n",
    "## OpenCV\n",
    "OpenCV is an open source library which provides tools to perform image and video processing for the computer vision, machine learning, and image processing applications. It is particularly popular for real-time operations which is very important in today’s systems. Integration with various libraries, such as Numpuy and python resulted in great capablities of processing the OpenCV array structures for analysis and mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sort import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the model:**\n",
    "OpenCV is one of the best computer vision libraries and it has functionalities for running deep learning inference. The OpenCV DNN module supports deep learning inference on images and videos. However, it does not support fine-tuning and training. \n",
    "\n",
    "OpenCV DNN module is highly optimized for Intel processors and can achieve high FPS when running inference on real-time videos for object detection and image segmentation applications. Here, a high FPS with the DNN module is carried out using pre-trained YOLOv3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "conf_threshold = 0.3\n",
    "nms_threshold = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to check the intersection with defined lines in the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return true if line segments AB and CD intersect\n",
    "def intersect(A,B,C,D):\n",
    "    return ccw(A,C,D) != ccw(B,C,D) and ccw(A,B,C) != ccw(A,B,D)\n",
    "\n",
    "def ccw(A,B,C):\n",
    "    return (C[1]-A[1]) * (B[0]-A[0]) > (B[1]-A[1]) * (C[0]-A[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a traffic video stream of a highway by creating a cv2.VideoCapture object which is a class for video capturing from video files, image sequences, or cameras. Then priting the info of total frames in video\n",
    "\n",
    "Processing a video means, performing operations on the video frame by frame. Frames are nothing but just the particular instance of the video in a single point of time. Therefore, can be treated as an regular image.\n",
    "\n",
    "The operations on frames started with vs.read() which returns a bool (True/False). If the frame is read correctly, it will be True. So you can check for the end of the video by checking this returned value.\n",
    "\n",
    "Each frame is processed similar to an image and the object detection steps are similar to what was described in my previous project **Image Object Detector** (see the following github link)\n",
    "https://github.com/majid-hosseini/Image-Object-Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 812 total frames in video\n"
     ]
    }
   ],
   "source": [
    "# initialize the video stream, pointer to output video file, and\n",
    "# frame dimensions\n",
    "vs = cv2.VideoCapture(\"input/highway.mp4\")\n",
    "_,frame= vs.read()\n",
    "(H, W) = frame.shape[:2]\n",
    "\n",
    "writer = None\n",
    "frameIndex = 1\n",
    "\n",
    "# try to determine the total number of frames in the video file\n",
    "try:\n",
    "    prop = cv2.cv.CV_CAP_PROP_FRAME_COUNT if imutils.is_cv2() \\\n",
    "        else cv2.CAP_PROP_FRAME_COUNT\n",
    "    total = int(vs.get(prop))\n",
    "    print(\"[INFO] {} total frames in video\".format(total))\n",
    "\n",
    "# an error occurred while trying to determine the total\n",
    "# number of frames in the video file\n",
    "except:\n",
    "    print(\"[INFO] could not determine # of frames in video\")\n",
    "    print(\"[INFO] no approx. completion time can be provided\")\n",
    "    total = -1\n",
    "    \n",
    "cross_check = []\n",
    "tracker = Sort()\n",
    "memory = {}\n",
    "time_test = {}\n",
    "time_for_speed = []\n",
    "\n",
    "dict_id_speed = {}\n",
    "counter1 = 0\n",
    "counter2 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the two lines on both directions of the highway for tracking of vehicles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = [(170, int(0.7*H)), (W//2 - 70, int(0.7*H))]\n",
    "    \n",
    "line2 = [(W//2+30, int(0.55*H)), (W - 350, int(0.55*H))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of AI traffic counter\n",
    "**This AI traffic counter is composed of three main components: a detector, tracker and counter.**\n",
    "\n",
    "- A detector capable of processing a Realtime video to identify vehicles in a given frame of video and returns a list of bounding boxes around the objects was explained in my previous project titled [Realtime Object Detector](https://github.com/majid-hosseini/Realtime-Object-Detector). \n",
    "- The tracker uses the bounding boxes to track the vehicles in subsequent frames. The detector is also used to update the trackers periodically to ensure that they are still tracking the vehicles correctly. \n",
    "- Once the objects are detected and tracked over different frames of a traffic video stream, a mathematical calculation is applied to count the number of vehicles that their previous and current frame positions intersect with a defined line in the frames.\n",
    "\n",
    "### Display of Processed Video\n",
    "The processed frames containing bounding boxed and class names around each detected object is displayed using cv2.imshow method. We specify a window name as a first argument, and the frame we would like to display as a second. The time between consecutive frame in display window is defined by cv.waitKey(). An small value results in a fast video display and a large value produce a show motion video display.\n",
    "\n",
    "### Saving a Video\n",
    "We processed a video frame-by-frame and plotted bounding box and the class name around each detected object, now we want to save that video. For images, it is very simple: just use cv.imwrite(). Here, a little more work is required.\n",
    "\n",
    "This time we create a VideoWriter object. We should specify the output file name (eg: output.avi). Then we should specify the FourCC code. Then number of frames per second (fps) and frame size should be passed. And the last one is the isColor flag. If it is True, the encoder expect color frame, otherwise it works with grayscale frame.\n",
    "\n",
    "cap.release() and cv2.destroyAllWindows() are the methods to close video files or the capturing device, and destroy the window, which was created by the imshow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] single frame took 3.6262 seconds\n",
      "[INFO] estimated total time to finish: 2944.4531\n",
      "[INFO] cleaning up...\n"
     ]
    }
   ],
   "source": [
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # read the next frame from the file\n",
    "    (grabbed, frame) = vs.read()    \n",
    "    if not grabbed:\n",
    "        break\n",
    "    \n",
    "    # construct a blob from the input frame and then perform a forward\n",
    "    # pass of the YOLO object detector, giving us our bounding boxes\n",
    "    # and associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (256, 256), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    layerOutputs = net.forward(ln)\n",
    "    end = time.time()\n",
    "    \n",
    "    # initialize our lists of detected bounding boxes, confidences,\n",
    "    # and class IDs, respectively\n",
    "    boxes = []\n",
    "    center = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "    \n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            # extract the class ID and confidence (i.e., probability)\n",
    "            # of the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "    \n",
    "            # filter out weak predictions by ensuring the detected\n",
    "            # probability is greater than the minimum probability\n",
    "            if confidence > conf_threshold:\n",
    "                # scale the bounding box coordinates back relative to\n",
    "                # the size of the image, keeping in mind that YOLO\n",
    "                # actually returns the center (x, y)-coordinates of\n",
    "                # the bounding box followed by the boxes' width and\n",
    "                # height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                    \n",
    "                # use the center (x, y)-coordinates to derive the top\n",
    "                # and and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "    \n",
    "                # update our list of bounding box coordinates,\n",
    "                # confidences, and class IDs\n",
    "                center.append(int(centerY))\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "                    \n",
    "    # apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "        \n",
    "    dets = []\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            dets.append([x, y, x+w, y+h, confidences[i]])\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "    dets = np.asarray(dets)\n",
    "    tracks = tracker.update(dets)\n",
    "        \n",
    "    boxes = []\n",
    "    indexIDs = []\n",
    "    c = []\n",
    "        \n",
    "    previous = memory.copy()\n",
    "    memory = {}\n",
    "    \n",
    "    for track in tracks:\n",
    "        boxes.append([track[0], track[1], track[2], track[3]])\n",
    "        indexIDs.append(int(track[4]))\n",
    "        memory[indexIDs[-1]] = boxes[-1]\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i]\n",
    "        \n",
    "        # extract the bounding box coordinates\n",
    "        (x, y) = (int(box[0]), int(box[1]))\n",
    "        (w, h) = (int(box[2]), int(box[3]))\n",
    "            \n",
    "        # draw a bounding box rectangle and label on the image\n",
    "        color = (0,100,255)\n",
    "        cv2.rectangle(frame, (x, y), (w, h), color, 2)\n",
    "        \n",
    "        if indexIDs[i] in previous:\n",
    "            previous_box = previous[indexIDs[i]]\n",
    "            (x2, y2) = (int(previous_box[0]), int(previous_box[1]))\n",
    "            (w2, h2) = (int(previous_box[2]), int(previous_box[3]))\n",
    "            p0 = (int(x + (w-x)/2), int(y + (h-y)/2))\n",
    "            p1 = (int(x2 + (w2-x2)/2), int(y2 + (h2-y2)/2))\n",
    "            #cv2.line(frame, p0, p1, color, 3)\n",
    "            id = indexIDs[i] \n",
    "            \n",
    "            #########################################################\n",
    "            label = str(classes[classIDs[i]])\n",
    "            font = cv2.FONT_HERSHEY_PLAIN\n",
    "            cv2.putText(frame, label, (x, y - 5), font, 1.0, color, 2)\n",
    "            if intersect(p0, p1, line1[0], line1[1]) and indexIDs[i] not in cross_check:\n",
    "                counter1 += 1\n",
    "                cross_check.append(indexIDs[i])\n",
    "\n",
    "            if intersect(p0, p1, line2[0], line2[1]) and indexIDs[i] not in cross_check:\n",
    "                counter2 += 1\n",
    "                cross_check.append(indexIDs[i])\n",
    "                \n",
    "    cv2.line(frame, line1[0], line1[1], (0, 0, 255), 3)\n",
    "    cv2.line(frame, line2[0], line2[1], (0, 210, 255), 3)\n",
    "    ##############################################\n",
    "    # draw counter\n",
    "    counter_text = \"Counter:{}\".format(counter1)\n",
    "    counter_text_2 = \"Counter:{}\".format(counter2)\n",
    "    font = cv2.FONT_HERSHEY_TRIPLEX\n",
    "    cv2.putText(frame, counter_text, (100,100), font, 2, (0, 0, 255), 4)\n",
    "    cv2.putText(frame, counter_text_2, (H-10,100), font, 2, (0, 210, 255), 4)\n",
    "    cv2.imshow(\"Image\", frame)\n",
    "        \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "            \n",
    "    # check if the video writer is None\n",
    "    if writer is None:\n",
    "        # initialize our video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(\"output.mp4\", fourcc, 30,(1080, 720), True)\n",
    "\n",
    "        # some information on processing single frame\n",
    "        if total > 0:\n",
    "            elap = (end - start)\n",
    "            print(\"[INFO] single frame took {:.4f} seconds\".format(elap))\n",
    "            print(\"[INFO] estimated total time to finish: {:.4f}\".format(\n",
    "                elap * total))\n",
    "\n",
    "    # write the output frame to disk\n",
    "    #new_dim = (1080,720)\n",
    "    new_dim = (W,H)\n",
    "    writer.write(cv2.resize(frame,new_dim, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "    # increase frame index\n",
    "    frameIndex += 1\n",
    "\n",
    "\n",
    "print(\"[INFO] cleaning up...\")\n",
    "writer.release()\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
